{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ab649ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T16:41:20.264068Z",
     "start_time": "2022-11-29T16:41:20.255114Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125bf68d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T15:20:01.343620Z",
     "start_time": "2022-11-29T15:20:01.333345Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Return a BeautifulSoup from the input `url`.\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    proxies = {'http': 'http://206.189.157.23'}\n",
    "    soup = bs4.BeautifulSoup(\n",
    "        requests.get(url, headers=headers, proxies=proxies).content\n",
    "    )\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a40e1b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T19:14:23.280727Z",
     "start_time": "2022-11-29T17:54:55.452191Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_df(soup_):\n",
    "    link_list = []\n",
    "    title_list = []\n",
    "    text_list = []\n",
    "    year_list = []\n",
    "    date_list = []\n",
    "    for soup in soup_:\n",
    "        # get title\n",
    "        title_list.append(soup.find('h2').get_text())\n",
    "        # get url\n",
    "        url_link = soup.find('h2').find('a').get('href')\n",
    "        link_list.append(url_link)\n",
    "        # get date when news article was published\n",
    "        soup_url1 = get_soup(url_link)\n",
    "        scrape_date = soup_url1.find('div', id='art_plat').get_text()\n",
    "        try:\n",
    "            date = re.findall(r' Philippine Daily Inquirer / [\\d][\\d][\\:]'\n",
    "                              r'[\\d][\\d][\\s][AP][M][\\s](\\w.+)',\n",
    "                              scrape_date)[0]\n",
    "        except:\n",
    "            date = re.findall(r'[\\d][\\d][\\s][AP][M][\\s](\\w.+)',\n",
    "                              scrape_date)[0]\n",
    "        date_ = datetime.strptime(date, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "        date_list.append(date_)\n",
    "        # get year\n",
    "        year = int(re.findall(r'\\w.+(\\d\\d\\d\\d)', date)[0])\n",
    "        year_list.append(year)\n",
    "        # get text\n",
    "        text = ' '.join([p.text.strip()\n",
    "                     for p in soup_url1\n",
    "                     .find_all('p')])\n",
    "#         try:\n",
    "#             text_ = re.findall(r' (\\w.+)\\sRELATED', text)[0]\n",
    "#         except:\n",
    "#             try:\n",
    "#                 text_ = re.findall(r' (\\w.+)\\sSubscribe to INQUIRER', text)[0]\n",
    "#             except:\n",
    "#                 text_ = re.findall(r'(\\w.+)\\sâ€”', text)[0]\n",
    "        text_list.append(text)\n",
    "    # convert to df\n",
    "    dict_ = {'title': title_list,\n",
    "             'link': link_list,\n",
    "             'date': date_list,\n",
    "             'year': year_list,\n",
    "             'text': text_list}\n",
    "    df = pd.DataFrame(dict_)\n",
    "    return df\n",
    "\n",
    "url_metro = 'https://newsinfo.inquirer.net/category/inquirer-headlines/metro'\n",
    "soup_0 = get_soup(url_metro)\n",
    "soup_ = soup_0.find_all('div', id='ch-ls-head')\n",
    "# metro news from latest page\n",
    "df_first = get_df(soup_)\n",
    "\n",
    "# metro news from first next page\n",
    "url_new =soup_0.find_all('div', id='ch-more-news')[0].find('a').get('href')\n",
    "soup_new = get_soup(url_new)\n",
    "soup_ = soup_new.find_all('div', id='ch-ls-head')\n",
    "df_new = get_df(soup_)\n",
    "df_new = pd.concat([df_first, df_new], axis=0, ignore_index=True)\n",
    "\n",
    "while True:\n",
    "    date1 = soup_[-1].find('div', id=\"ch-postdate\").find('span').get_text()\n",
    "    date_1 = datetime.strptime(date1, '%B %d, %Y').strftime('%Y-%m-%d')\n",
    "    if date_1 >= '2016-06-30':\n",
    "        url_new =soup_new.find_all('div', id='ch-more-news')[0].find_all('a')[1].get('href')\n",
    "        soup_new = get_soup(url_new)\n",
    "        soup_ = soup_new.find_all('div', id='ch-ls-head')\n",
    "        df_newnew = get_df(soup_)\n",
    "        df_new = pd.concat([df_new, df_newnew], axis=0, ignore_index=True)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0c10808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T20:32:28.706463Z",
     "start_time": "2022-11-29T20:32:28.195062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the dataset to a csv file\n",
    "df_new.to_csv('all_scraped_metro_news.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
